{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "\n",
    "# Reinforcement learning\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecEnvWrapper, VecMonitor, VecVideoRecorder\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logging\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "\n",
    "# Project-specific imports\n",
    "sys.path.insert(0, '../src')\n",
    "from replay_buffer import HDF5ReplayBufferRAM\n",
    "from models import Autoencoder, CTR_Attention_dil\n",
    "from rl_wrappers import RawRewardTracker\n",
    "from rl_callbacks import RawRewardLoggingCallback, WandbModelCheckpointCallback\n",
    "from rl_networks import AutoencoderFeatureExtractor\n",
    "from rl_policies import MyDQNPolicy\n",
    "from rl_buffers import MyReplayBuffer, PrioritizedReplayBuffer\n",
    "from rl_models import MyDQNModel\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_dtype(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468181e-5475-4e67-849d-fae42d89cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muthenrik\u001b[0m (\u001b[33muthenrik-the-university-of-tokyo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/TCDS/wandb/run-20250812_164303-3v9mi3o4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uthenrik-the-university-of-tokyo/Covert%20Attention%20Agents/runs/3v9mi3o4' target=\"_blank\">scarlet-feather-327</a></strong> to <a href='https://wandb.ai/uthenrik-the-university-of-tokyo/Covert%20Attention%20Agents' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uthenrik-the-university-of-tokyo/Covert%20Attention%20Agents' target=\"_blank\">https://wandb.ai/uthenrik-the-university-of-tokyo/Covert%20Attention%20Agents</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uthenrik-the-university-of-tokyo/Covert%20Attention%20Agents/runs/3v9mi3o4' target=\"_blank\">https://wandb.ai/uthenrik-the-university-of-tokyo/Covert%20Attention%20Agents/runs/3v9mi3o4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.1+2750686)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to runs/3v9mi3o4/DQN_0\n",
      "Saving video to /workspace/TCDS/videos/3v9mi3o4/rl-video-step-0-to-step-300.mp4\n",
      "MoviePy - Building video /workspace/TCDS/videos/3v9mi3o4/rl-video-step-0-to-step-300.mp4.\n",
      "MoviePy - Writing video /workspace/TCDS/videos/3v9mi3o4/rl-video-step-0-to-step-300.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /workspace/TCDS/videos/3v9mi3o4/rl-video-step-0-to-step-300.mp4\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    current_PER_beta | 0.4      |\n",
      "|    current_lambda   | 0.19     |\n",
      "|    ep_len_mean      | 2.04e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.971    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 277      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 8174     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    current_PER_beta | 0.4      |\n",
      "|    current_lambda   | 0.19     |\n",
      "|    ep_len_mean      | 2.04e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.941    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 285      |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 16355    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    current_PER_beta | 0.4      |\n",
      "|    current_lambda   | 0.19     |\n",
      "|    ep_len_mean      | 2.04e+03 |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.912    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 288      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 24534    |\n",
      "----------------------------------\n",
      "-------------------------------------\n",
      "| rollout/            |             |\n",
      "|    current_PER_beta | 0.402       |\n",
      "|    current_lambda   | 0.19        |\n",
      "|    ep_len_mean      | 2.04e+03    |\n",
      "|    ep_rew_mean      | 0           |\n",
      "|    exploration_rate | 0.882       |\n",
      "| time/               |             |\n",
      "|    episodes         | 16          |\n",
      "|    fps              | 134         |\n",
      "|    time_elapsed     | 243         |\n",
      "|    total_timesteps  | 32712       |\n",
      "| train/              |             |\n",
      "|    learning_rate    | 0.00025     |\n",
      "|    loss             | 6.36e-07    |\n",
      "|    n_updates        | 1927        |\n",
      "|    q_values/mean    | -0.06957481 |\n",
      "|    q_values/std     | 0.005473332 |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/            |               |\n",
      "|    current_PER_beta | 0.404         |\n",
      "|    current_lambda   | 0.19          |\n",
      "|    ep_len_mean      | 2.04e+03      |\n",
      "|    ep_rew_mean      | 0             |\n",
      "|    exploration_rate | 0.853         |\n",
      "| time/               |               |\n",
      "|    episodes         | 20            |\n",
      "|    fps              | 99            |\n",
      "|    time_elapsed     | 412           |\n",
      "|    total_timesteps  | 40897         |\n",
      "| train/              |               |\n",
      "|    learning_rate    | 0.00025       |\n",
      "|    loss             | 2.58e-08      |\n",
      "|    n_updates        | 3974          |\n",
      "|    q_values/mean    | -0.049123075  |\n",
      "|    q_values/std     | 0.00058218004 |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/            |              |\n",
      "|    current_PER_beta | 0.406        |\n",
      "|    current_lambda   | 0.19         |\n",
      "|    ep_len_mean      | 2.04e+03     |\n",
      "|    ep_rew_mean      | 0            |\n",
      "|    exploration_rate | 0.823        |\n",
      "| time/               |              |\n",
      "|    episodes         | 24           |\n",
      "|    fps              | 90           |\n",
      "|    time_elapsed     | 544          |\n",
      "|    total_timesteps  | 49069        |\n",
      "| train/              |              |\n",
      "|    learning_rate    | 0.00025      |\n",
      "|    loss             | 7.3e-08      |\n",
      "|    n_updates        | 6017         |\n",
      "|    q_values/mean    | -0.048435062 |\n",
      "|    q_values/std     | 0.0005341144 |\n",
      "--------------------------------------\n",
      "Saving video to /workspace/TCDS/videos/3v9mi3o4/rl-video-step-50000-to-step-50300.mp4\n",
      "MoviePy - Building video /workspace/TCDS/videos/3v9mi3o4/rl-video-step-50000-to-step-50300.mp4.\n",
      "MoviePy - Writing video /workspace/TCDS/videos/3v9mi3o4/rl-video-step-50000-to-step-50300.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /workspace/TCDS/videos/3v9mi3o4/rl-video-step-50000-to-step-50300.mp4\n",
      "--------------------------------------\n",
      "| rollout/            |              |\n",
      "|    current_PER_beta | 0.408        |\n",
      "|    current_lambda   | 0.19         |\n",
      "|    ep_len_mean      | 2.04e+03     |\n",
      "|    ep_rew_mean      | 0            |\n",
      "|    exploration_rate | 0.794        |\n",
      "| time/               |              |\n",
      "|    episodes         | 28           |\n",
      "|    fps              | 85           |\n",
      "|    time_elapsed     | 673          |\n",
      "|    total_timesteps  | 57256        |\n",
      "| train/              |              |\n",
      "|    learning_rate    | 0.00025      |\n",
      "|    loss             | 5.02e-06     |\n",
      "|    n_updates        | 8063         |\n",
      "|    q_values/mean    | -0.047702737 |\n",
      "|    q_values/std     | 0.0008837336 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/            |              |\n",
      "|    current_PER_beta | 0.41         |\n",
      "|    current_lambda   | 0.19         |\n",
      "|    ep_len_mean      | 2.05e+03     |\n",
      "|    ep_rew_mean      | 0            |\n",
      "|    exploration_rate | 0.764        |\n",
      "| time/               |              |\n",
      "|    episodes         | 32           |\n",
      "|    fps              | 81           |\n",
      "|    time_elapsed     | 804          |\n",
      "|    total_timesteps  | 65441        |\n",
      "| train/              |              |\n",
      "|    learning_rate    | 0.00025      |\n",
      "|    loss             | 1.57e-06     |\n",
      "|    n_updates        | 10110        |\n",
      "|    q_values/mean    | -0.04788319  |\n",
      "|    q_values/std     | 0.0017084512 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/            |              |\n",
      "|    current_PER_beta | 0.412        |\n",
      "|    current_lambda   | 0.19         |\n",
      "|    ep_len_mean      | 2.04e+03     |\n",
      "|    ep_rew_mean      | 0            |\n",
      "|    exploration_rate | 0.735        |\n",
      "| time/               |              |\n",
      "|    episodes         | 36           |\n",
      "|    fps              | 78           |\n",
      "|    time_elapsed     | 935          |\n",
      "|    total_timesteps  | 73615        |\n",
      "| train/              |              |\n",
      "|    learning_rate    | 0.00025      |\n",
      "|    loss             | 7.1e-07      |\n",
      "|    n_updates        | 12153        |\n",
      "|    q_values/mean    | -0.047705367 |\n",
      "|    q_values/std     | 0.0021337902 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/            |              |\n",
      "|    current_PER_beta | 0.414        |\n",
      "|    current_lambda   | 0.19         |\n",
      "|    ep_len_mean      | 2.04e+03     |\n",
      "|    ep_rew_mean      | 0            |\n",
      "|    exploration_rate | 0.706        |\n",
      "| time/               |              |\n",
      "|    episodes         | 40           |\n",
      "|    fps              | 77           |\n",
      "|    time_elapsed     | 1061         |\n",
      "|    total_timesteps  | 81792        |\n",
      "| train/              |              |\n",
      "|    learning_rate    | 0.00025      |\n",
      "|    loss             | 1.7e-07      |\n",
      "|    n_updates        | 14197        |\n",
      "|    q_values/mean    | -0.047548167 |\n",
      "|    q_values/std     | 0.00219706   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/            |              |\n",
      "|    current_PER_beta | 0.416        |\n",
      "|    current_lambda   | 0.19         |\n",
      "|    ep_len_mean      | 2.04e+03     |\n",
      "|    ep_rew_mean      | 0            |\n",
      "|    exploration_rate | 0.676        |\n",
      "| time/               |              |\n",
      "|    episodes         | 44           |\n",
      "|    fps              | 76           |\n",
      "|    time_elapsed     | 1182         |\n",
      "|    total_timesteps  | 89970        |\n",
      "| train/              |              |\n",
      "|    learning_rate    | 0.00025      |\n",
      "|    loss             | 1.97e-07     |\n",
      "|    n_updates        | 16242        |\n",
      "|    q_values/mean    | -0.048282463 |\n",
      "|    q_values/std     | 0.002122032  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/            |              |\n",
      "|    current_PER_beta | 0.418        |\n",
      "|    current_lambda   | 0.19         |\n",
      "|    ep_len_mean      | 2.04e+03     |\n",
      "|    ep_rew_mean      | 0            |\n",
      "|    exploration_rate | 0.647        |\n",
      "| time/               |              |\n",
      "|    episodes         | 48           |\n",
      "|    fps              | 75           |\n",
      "|    time_elapsed     | 1296         |\n",
      "|    total_timesteps  | 98149        |\n",
      "| train/              |              |\n",
      "|    learning_rate    | 0.00025      |\n",
      "|    loss             | 1.88e-07     |\n",
      "|    n_updates        | 18287        |\n",
      "|    q_values/mean    | -0.047059752 |\n",
      "|    q_values/std     | 0.0021263673 |\n",
      "--------------------------------------\n",
      "Saving video to /workspace/TCDS/videos/3v9mi3o4/rl-video-step-100000-to-step-100300.mp4\n",
      "MoviePy - Building video /workspace/TCDS/videos/3v9mi3o4/rl-video-step-100000-to-step-100300.mp4.\n",
      "MoviePy - Writing video /workspace/TCDS/videos/3v9mi3o4/rl-video-step-100000-to-step-100300.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /workspace/TCDS/videos/3v9mi3o4/rl-video-step-100000-to-step-100300.mp4\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "use_wandb = True\n",
    "game_name = \"Freeway\"\n",
    "\n",
    "# Lambda values for 16% attention target\n",
    "lam_set_dict = {\n",
    "    'Enduro': np.float64(0.15636971314004802),\n",
    "    'Freeway': np.float64(0.1899595360666595),\n",
    "    'MsPacman': np.float64(0.17380204890481837),\n",
    "    'Seaquest': np.float64(0.22773814757646266),\n",
    "    'SpaceInvaders': np.float64(0.16151139168983497),\n",
    "    'Riverraid': np.float64(0.15986161207261196)\n",
    "}\n",
    "\n",
    "# Game mappings\n",
    "games = ['Enduro', 'Freeway', 'MsPacman', 'Riverraid', 'Seaquest', 'SpaceInvaders']\n",
    "AA_to_AH = {\n",
    "    'Enduro': 'enduro',\n",
    "    'Freeway': 'freeway',\n",
    "    'MsPacman': 'ms_pacman',\n",
    "    'Riverraid': 'riverraid',\n",
    "    'Seaquest': 'seaquest',\n",
    "    'SpaceInvaders': 'space_invaders'\n",
    "}\n",
    "\n",
    "# Training loop over seeds and inverse_psi settings\n",
    "for i_seed in [0, 1, 2]:\n",
    "    for inv_psi in [False, True]:\n",
    "        \n",
    "        # Training configuration\n",
    "        config = {\n",
    "            \"game_name\": game_name,\n",
    "            \"use_CTR\": True,\n",
    "            \"env_seed\": [42, 1337, 30890][i_seed],\n",
    "            \"total_timesteps\": 2_500_000,  # Frameskip=4, so baseline is 2.5M for 10M Frames\n",
    "            \"learning_starts\": 25_000,  # 10k-25k is baseline\n",
    "            \"train_freq\": 4,  # 4 is baseline\n",
    "            \"gamma\": 0.99,  # Standard is 0.99, Human-like is <=0.9\n",
    "            \"use_PER\": True,  # Prioritized Experience Replay\n",
    "            \"inverse_psi\": inv_psi,\n",
    "            \"lambda_strategy\": \"fixed\",\n",
    "            \"lambda_fixed_val\": lam_set_dict[game_name],\n",
    "            \"beta_res_attention\": 0.0,  # How much of non-attended features to retain\n",
    "            \"lin_sched_lam_min\": lam_set_dict[game_name],\n",
    "            \"lin_sched_lam_max\": 1.0,\n",
    "            \"lin_sched_max_timesteps\": 2_000_000,\n",
    "            \"model_save_freq\": 100_000,\n",
    "        }\n",
    "        \n",
    "        # Lambda scheduler setup\n",
    "        if config[\"lambda_strategy\"] == \"fixed\":\n",
    "            lam_scheduler = lambda step: config[\"lambda_fixed_val\"] * np.ones_like(step)\n",
    "        elif config[\"lambda_strategy\"] == \"linear_scheduler\":\n",
    "            lam_scheduler = lambda step: np.clip(\n",
    "                config[\"lin_sched_lam_min\"] + (config[\"lin_sched_lam_max\"] - config[\"lin_sched_lam_min\"]) * step / config[\"lin_sched_max_timesteps\"],\n",
    "                config[\"lin_sched_lam_min\"],\n",
    "                config[\"lin_sched_lam_max\"]\n",
    "            )\n",
    "        elif config[\"lambda_strategy\"] == \"max_to_min\":\n",
    "            lam_scheduler = lambda step: np.clip(\n",
    "                config[\"lin_sched_lam_max\"] - (config[\"lin_sched_lam_max\"] - config[\"lin_sched_lam_min\"]) * step / config[\"lin_sched_max_timesteps\"],\n",
    "                config[\"lin_sched_lam_min\"],\n",
    "                config[\"lin_sched_lam_max\"]\n",
    "            )\n",
    "        \n",
    "        # Initialize WandB\n",
    "        if use_wandb:\n",
    "            run = wandb.init(\n",
    "                entity=\"uthenrik-the-university-of-tokyo\",\n",
    "                project=\"Covert Attention Agents\",\n",
    "                config=config,\n",
    "                sync_tensorboard=True,\n",
    "                monitor_gym=True,\n",
    "            )\n",
    "        \n",
    "        # Load pre-trained models\n",
    "        game_name = config['game_name']\n",
    "        \n",
    "        # Load autoencoder\n",
    "        ae_pattern = f'trained_models/autoencoder/AE_4f_TCDS_BlurPool64_{game_name}_*.pkl'\n",
    "        ae_files = glob.glob(ae_pattern)\n",
    "        checkpoint_path = max(ae_files, key=os.path.getmtime)\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=True, map_location=device)\n",
    "        autoencoder = Autoencoder(device).to(device)\n",
    "        autoencoder.load_state_dict(checkpoint['state'])\n",
    "        autoencoder_copy = copy.deepcopy(autoencoder)\n",
    "        \n",
    "        for param in autoencoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Load CTR attention models\n",
    "        CTR_version = 'V15'\n",
    "        ctr_pattern = f'trained_models/CTR_att/{CTR_version}/nCTR_AA_AH_{CTR_version}_{game_name}_*.pkl'\n",
    "        ctr_files = glob.glob(ctr_pattern)\n",
    "        checkpoint_path = max(ctr_files, key=os.path.getmtime)\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=True, map_location=device)\n",
    "        \n",
    "        CTR_AA = CTR_Attention_dil(repr_shape=(1, 32, 21, 21), autoencoder=copy.deepcopy(autoencoder), device=device).to(device)\n",
    "        CTR_AA.load_state_dict(checkpoint['CTR_AA_state'], strict=False)\n",
    "        CTR_AA.eval()\n",
    "        \n",
    "        CTR_AH = CTR_Attention_dil(repr_shape=(1, 32, 21, 21), autoencoder=copy.deepcopy(autoencoder), device=device).to(device)\n",
    "        CTR_AH.load_state_dict(checkpoint['CTR_AH_state'], strict=False)\n",
    "        CTR_AH.eval()\n",
    "        \n",
    "        # Environment setup\n",
    "        SEED = config['env_seed']\n",
    "        \n",
    "        def make_env():\n",
    "            def _init():\n",
    "                env = gym.make(f\"ALE/{game_name}-v5\", frameskip=1, render_mode=\"rgb_array\")\n",
    "                env = RawRewardTracker(env)\n",
    "                env = AtariWrapper(env, frame_skip=4, terminal_on_life_loss=False)\n",
    "                env.reset(seed=SEED)\n",
    "                env.action_space.seed(SEED)\n",
    "                return env\n",
    "            return _init\n",
    "        \n",
    "        set_random_seed(SEED)\n",
    "        \n",
    "        env = DummyVecEnv([make_env()])\n",
    "        env.seed(SEED)\n",
    "        env = VecMonitor(env)\n",
    "        \n",
    "        if use_wandb:\n",
    "            env = VecVideoRecorder(\n",
    "                env,\n",
    "                f\"videos/{run.id}\",\n",
    "                record_video_trigger=lambda x: x % 50000 == 0,\n",
    "                video_length=300,\n",
    "            )\n",
    "        \n",
    "        env = VecFrameStack(env, n_stack=4)\n",
    "        \n",
    "        # Model creation\n",
    "        def make_dqn_model(env, use_CTR):\n",
    "            policy_kwargs = dict(\n",
    "                features_extractor_class=AutoencoderFeatureExtractor,\n",
    "                features_extractor_kwargs=dict(\n",
    "                    autoencoder=autoencoder,\n",
    "                    lam_scheduler=lam_scheduler,\n",
    "                    device=device,\n",
    "                    CTR=CTR_AH,\n",
    "                    config=config\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if config[\"use_PER\"]:\n",
    "                replay_buffer_class = PrioritizedReplayBuffer\n",
    "            else:\n",
    "                replay_buffer_class = MyReplayBuffer\n",
    "        \n",
    "            dqn_kwargs = dict(\n",
    "                env=env,\n",
    "                policy=MyDQNPolicy,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                replay_buffer_class=replay_buffer_class,\n",
    "                replay_buffer_kwargs={\n",
    "                    \"handle_timeout_termination\": False,\n",
    "                    \"config\": config,\n",
    "                },\n",
    "                verbose=1,\n",
    "                device=device,\n",
    "                learning_rate=2.5e-4,\n",
    "                buffer_size=500_000,\n",
    "                batch_size=32,\n",
    "                gamma=config[\"gamma\"],\n",
    "                exploration_initial_eps=1.0,\n",
    "                exploration_final_eps=0.1,\n",
    "                exploration_fraction=0.1,\n",
    "                target_update_interval=10_000,\n",
    "                train_freq=config[\"train_freq\"],\n",
    "                gradient_steps=1,\n",
    "                learning_starts=config[\"learning_starts\"],\n",
    "                optimize_memory_usage=True,\n",
    "            )\n",
    "        \n",
    "            if use_wandb:\n",
    "                dqn_kwargs[\"tensorboard_log\"] = f\"runs/{run.id}\"\n",
    "        \n",
    "            return MyDQNModel(**dqn_kwargs)\n",
    "        \n",
    "        # Create and configure model\n",
    "        model_DDQN = make_dqn_model(env, use_CTR=config['use_CTR'])\n",
    "        model_DDQN.config = config\n",
    "        model_DDQN.policy.step_counter = 0\n",
    "        model_DDQN.policy.set_CTR_strategy(config['use_CTR'])\n",
    "        \n",
    "        # Fix autoencoder references\n",
    "        model_DDQN.q_net.features_extractor.autoencoder = copy.deepcopy(autoencoder_copy)\n",
    "        model_DDQN.q_net.features_extractor.autoencoder.eval()\n",
    "        model_DDQN.q_net_target.features_extractor.autoencoder = copy.deepcopy(autoencoder_copy)\n",
    "        model_DDQN.q_net_target.features_extractor.autoencoder.eval()\n",
    "        \n",
    "        # Training\n",
    "        if use_wandb:\n",
    "            wandb.watch(model_DDQN.policy.q_net, log=\"all\", log_freq=1000)\n",
    "        \n",
    "        if use_wandb:\n",
    "            callbacks = CallbackList([\n",
    "                WandbCallback(),\n",
    "                RawRewardLoggingCallback(),\n",
    "                WandbModelCheckpointCallback(model_save_freq=config[\"model_save_freq\"]),\n",
    "            ])\n",
    "            model_DDQN.learn(total_timesteps=config[\"total_timesteps\"], reset_num_timesteps=False, callback=callbacks)\n",
    "        else:\n",
    "            model_DDQN.learn(total_timesteps=config[\"total_timesteps\"], reset_num_timesteps=False)\n",
    "        \n",
    "        wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
