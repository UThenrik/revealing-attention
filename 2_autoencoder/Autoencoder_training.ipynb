{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Project-specific imports\n",
    "sys.path.insert(0, './atarihead')\n",
    "sys.path.insert(0, '../src')\n",
    "from replay_buffer import HDF5ReplayBufferRAM\n",
    "from models import Autoencoder\n",
    "from utils import get_lr\n",
    "\n",
    "# Configuration\n",
    "pd.options.display.float_format = '{:.3g}'.format\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# IPython display setup\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Atarihead replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "games=['Alien',\n",
    "       'Asterix',\n",
    "       'BankHeist',\n",
    "       'Berzerk',\n",
    "       'Breakout',\n",
    "       'Centipede',\n",
    "       'DemonAttack',\n",
    "       'Enduro',\n",
    "       'Freeway',\n",
    "       'Frostbite',\n",
    "       'Hero',\n",
    "       'MontezumaRevenge',\n",
    "       'MsPacman',\n",
    "       'NameThisGame',\n",
    "       'Phoenix',\n",
    "       'Riverraid',\n",
    "       'RoadRunner',\n",
    "       'Seaquest',\n",
    "       'SpaceInvaders',\n",
    "       'Venture']\n",
    "\n",
    "n_actions={'Alien': 18,\n",
    "       'Asterix': 9,\n",
    "       'BankHeist': 18,\n",
    "       'Berzerk': 18,\n",
    "       'Breakout': 4,\n",
    "       'Centipede': 18,\n",
    "       'DemonAttack': 6,\n",
    "       'Enduro': 9,\n",
    "       'Freeway': 3,\n",
    "       'Frostbite': 18,\n",
    "       'Hero': 18,\n",
    "       'MontezumaRevenge': 18,\n",
    "       'MsPacman': 9,\n",
    "       'NameThisGame': 6,\n",
    "       'Phoenix': 8,\n",
    "       'Riverraid': 18,\n",
    "       'RoadRunner': 18,\n",
    "       'Seaquest': 18,\n",
    "       'SpaceInvaders': 6,\n",
    "       'Venture': 18}\n",
    "\n",
    "AA_to_AH={'Alien': 'alien',\n",
    "       'Asterix': 'asterix',\n",
    "       'BankHeist': 'bank_heist',\n",
    "       'Berzerk': 'berzerk',\n",
    "       'Breakout': 'breakout',\n",
    "       'Centipede': 'centipede',\n",
    "       'DemonAttack': 'demon_attack',\n",
    "       'Enduro': 'enduro',\n",
    "       'Freeway': 'freeway',\n",
    "       'Frostbite': 'frostbite',\n",
    "       'Hero': 'hero',\n",
    "       'MontezumaRevenge': 'montezuma_revenge',\n",
    "       'MsPacman': 'ms_pacman',\n",
    "       'NameThisGame': 'name_this_game',\n",
    "       'Phoenix': 'phoenix',\n",
    "       'Riverraid': 'riverraid',\n",
    "       'RoadRunner': 'road_runner',\n",
    "       'Seaquest': 'seaquest',\n",
    "       'SpaceInvaders': 'space_invaders',\n",
    "       'Venture': 'venture'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make game max reward!!!\n",
    "\n",
    "game_name='Seaquest' # Give name in AA style\n",
    "train_val_split=0.8\n",
    "\n",
    "memoryAA = HDF5ReplayBufferRAM.load(\n",
    "file_path=fr'replay_buffers/atari_agents/{game_name}_atari_agents_buffer_4f_84gray.h5',file_rw_option='r', train_val_split=train_val_split, RAM_ratio=1/4)\n",
    "memoryAH = HDF5ReplayBufferRAM.load(\n",
    "file_path=fr'replay_buffers/atarihead/{AA_to_AH[game_name]}_atarihead_buffer_all_4f_84gray.h5',file_rw_option='r', train_val_split=train_val_split, RAM_ratio=1/4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define AE training Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_training:\n",
    "    def __init__(self, lr):\n",
    "\n",
    "        self.n_samples=8\n",
    "        self.flag_load_ae = False\n",
    "                \n",
    "        self.autoencoder=Autoencoder(device).to(device)\n",
    "        self.ae_optimizer = optim.AdamW(self.autoencoder.parameters(), lr=lr,weight_decay=1e-2)\n",
    "\n",
    "    def train_AE_att(self,replay_bufferAA,replay_bufferAH,batch_size):\n",
    "\n",
    "        results=[[],[]]\n",
    "\n",
    "        stateAA_np_all, _, _, _, _, _, train_flagsAA = replay_bufferAA.sample_stacked_fwd(int(batch_size/train_val_split))\n",
    "        stateAA_all = torch.tensor(stateAA_np_all, dtype=torch.float32, device=device) / 255.0 * 2 - 1\n",
    "\n",
    "        stateAH_np_all, _, _, _, _, _, train_flagsAH = replay_bufferAH.sample_stacked_fwd(int(batch_size/train_val_split))\n",
    "        stateAH_all = torch.tensor(stateAH_np_all, dtype=torch.float32, device=device) / 255.0 * 2 - 1\n",
    "        \n",
    "        normal_criterion = nn.MSELoss()        \n",
    "        for sample_mode,stateAA,stateAH,id_train in zip(['train','val'],\n",
    "            [stateAA_all[train_flagsAA],stateAA_all[~train_flagsAA]],\n",
    "            [stateAH_all[train_flagsAH],stateAH_all[~train_flagsAH]],\n",
    "            [0,1]):\n",
    "\n",
    "            if sample_mode=='val':\n",
    "                self.autoencoder.eval()\n",
    "\n",
    "            stateAA_recr=self.autoencoder(stateAA)\n",
    "            stateAH_recr=self.autoencoder(stateAH)\n",
    "            \n",
    "            loss_auto=normal_criterion(stateAA,stateAA_recr)+normal_criterion(stateAH,stateAH_recr)\n",
    "\n",
    "            if sample_mode=='train':\n",
    "\n",
    "                self.ae_optimizer.zero_grad()\n",
    "                loss_auto.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.autoencoder.parameters(), 1.0)\n",
    "                self.ae_optimizer.step()\n",
    "            if sample_mode=='val':\n",
    "                self.autoencoder.train()\n",
    "\n",
    "            results[id_train]=loss_auto.item()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_trainer=AE_training( lr=1e-3)\n",
    "loss_df = pd.DataFrame(columns=['q-epoch','lr','t:ae','v:ae'])\n",
    "\n",
    "batch_size=64\n",
    "n_qepochs=800\n",
    "n_batches=100\n",
    "ratio_val=4\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(AE_trainer.ae_optimizer, step_size=600, gamma=0.1)\n",
    "\n",
    "AE_trainer.train_AE=True\n",
    "j_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m j_epoch \u001b[38;5;241m<\u001b[39mn_qepochs:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[0;32m----> 8\u001b[0m         ae\u001b[38;5;241m=\u001b[39m\u001b[43mAE_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_AE_att\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemoryAA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemoryAH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         ae\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(ae)\n\u001b[1;32m     10\u001b[0m         ae_sum\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mae\n",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m, in \u001b[0;36mAE_training.train_AE_att\u001b[0;34m(self, replay_bufferAA, replay_bufferAH, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 49\u001b[0m     results[id_train]\u001b[38;5;241m=\u001b[39m\u001b[43mloss_auto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ae_sum=0\n",
    "\n",
    "stop_training=False\n",
    "\n",
    "while j_epoch <n_qepochs:\n",
    "    for i in range(n_batches):\n",
    "\n",
    "        ae=AE_trainer.train_AE_att(memoryAA,memoryAH,batch_size)\n",
    "        ae=np.array(ae)\n",
    "        ae_sum+=ae\n",
    "\n",
    "\n",
    "    ae_mean=ae_sum/n_batches\n",
    "    current_lr = get_lr(AE_trainer.ae_optimizer)\n",
    "\n",
    "    loss_df.loc[j_epoch] = [j_epoch+1, current_lr, *ae_mean]\n",
    "    clear_output()\n",
    "    print(loss_df.tail(10).to_string(index=False))\n",
    "\n",
    "    ae_sum=0\n",
    "    \n",
    "    j_epoch+=1\n",
    "    scheduler.step()\n",
    "    memoryAA.shuffle_RAM()\n",
    "    memoryAH.shuffle_RAM()\n",
    "    print('RB RAM shuffled')\n",
    "    \n",
    "    if stop_training:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(7,5))\n",
    "ravg=5\n",
    "\n",
    "ax.plot(loss_df['t:ae'].rolling(ravg).mean(),label='train')\n",
    "ax.plot(loss_df['v:ae'].rolling(ravg).mean(),label='val')\n",
    "ax.set_title('AE loss')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_print=np.random.randint(batch_size)\n",
    "state_np, actions, next_state_np, rewards, frame_ids, gaze_positions, done_np,_ = memoryAA.sample_stacked(batch_size)\n",
    "state = torch.tensor(state_np,device=device,dtype=torch.float32)/255*2-1\n",
    "state_img = state_np.reshape(state_np.shape[0], state_np.shape[1], state_np.shape[2], -1)\n",
    "\n",
    "next_state_img=next_state_np\n",
    "gaze_pos = gaze_positions[i_print]\n",
    "\n",
    "fig,ax=plt.subplots(1,2)\n",
    "state_recr=AE_trainer.autoencoder(state)\n",
    "state_recr_img=((state_recr.detach().cpu().numpy()[...,-1]+1)/2*255).astype(np.uint8)\n",
    "\n",
    "ax[0].imshow(state_img[i_print][..., -1], interpolation='nearest')\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(state_recr_img[i_print], interpolation='nearest')\n",
    "ax[1].set_title('Recreated Image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save whole AE Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_descr='BlurPool64'\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "AE_checkpoint = {\n",
    "    'state': AE_trainer.autoencoder.state_dict(),\n",
    "    'optimizer_state': AE_trainer.ae_optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "with open(fr'trained_models/AE_4f_TCDS_{model_descr}_{game_name}_{now}.pkl', 'wb') as f:\n",
    "    torch.save(AE_checkpoint, f)\n",
    "\n",
    "loss_df.to_csv(fr'trained_models/AE_4f_TCDS_df_{model_descr}_{game_name}_{now}.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
